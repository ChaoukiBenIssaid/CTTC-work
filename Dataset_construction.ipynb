{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# <center>**MIMO NOMA with two Clusters**<center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In this notebook, we will try to summarize the system model as well as the code used for the simulation part. We will provide comments and explanation for both parts to make it easy to reproduce the result or detect any bugs/errors in the code. The current work is part of a collobaration between CTTC and KAUST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **1. System Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The received signal is given by\n",
    "\n",
    "$$ \\bf{y = H x + n = H P s + n} $$\n",
    "\n",
    "\n",
    "where $\\textbf{H} \\in \\mathbb{C}^{N \\times K}$, $\\textbf{P} \\in \\mathbb{C}^{K \\times K}$, $ \\textbf{x} \\in \\mathbb{C}^{K \\times 1}$, $\\textbf{n} \\in \\mathbb{C}^{N \\times 1}$. Here, $N$ denotes the number of antennas at the base station (BS), and $K$ is the number of users.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The matrix $\\textbf{H} = [h_1, h_2, \\dots, h_K]$ where $h_k \\sim CN(0,\\textbf{I}_N) $. We can later on see how things will work if we consider the correlated case.\n",
    "\n",
    "The matrix $\\textbf{P}$ is defined as $\\textbf{P} = \\text{diag}(P_k)$, where $P_k = d_k^{-\\beta} 10^{\\frac{\\alpha_k}{10}}$ such that\n",
    "- $d_k^{-\\beta}$ is the path-loss where $d_k$ is the distance from the BS to the user $k$, and $\\beta$ is the path-loss exponent,\n",
    "- the lognormal fading exponent $\\alpha_k \\sim N(0,\\sigma_{\\alpha}^2)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The users are randomly distributed within a rectangle with dimension $(X_0,Y_0)$ from the BS, i.e., the distance $d_k = \\sqrt{X_k^2 + Y_k^2}$, where $X_k$ is a uniform RV with range $[0,X_0]$, and $Y_k$ is a uniform RV with range $[0,Y_0]$. This can be later on changed if the model of the distribution of the users is very simple.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Based on Xavi's notes, the output SNR of a user $i$ in the first cluster is given by\n",
    "\n",
    "$$ SNR_i^{(1)} = P_i \\bf{h_i^H} \\left(H_{(i)} P_{(i)} H_{(i)}^H + \\sigma^2 I \\right)^{-1} h_i$$\n",
    "\n",
    "If we assume that we divide the users into two clusters, $\\mathcal{I}_1$ and $\\mathcal{I}_1$, the sum rate associated with the first stage is given by\n",
    "\n",
    "$$ R_1 = \\sum_{k \\in \\mathcal{I}_1}{\\log\\left(1 + SNR_k^{(1)}\\right)}$$\n",
    "\n",
    "The rate of the second stage is given by\n",
    "\n",
    "$$ R_2 = \\sum_{k \\in \\mathcal{I}_2}{\\log\\left(1 + SNR_k^{(2)}\\right)},$$\n",
    "\n",
    "where the SNR of user $i$ in the second cluster is given by\n",
    "\n",
    "$$ SNR_i^{(2)} = P_i \\bf{h_i^H} \\left(H_{(\\mathcal{I}_1 \\cup \\{i\\})} P_{(\\mathcal{I}_1 \\cup \\{i\\})} H_{(\\mathcal{I}_1 \\cup \\{i\\})}^H + \\sigma^2 I \\right)^{-1} h_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The best clustering strategy will be base on maximizing the sum rate $R_1 + R_2$, other possible criteria are maximizing the minimum between $R_1$ and $R_2$ or the product $R_1 \\times R_2$. First, we will focus on the first criterion, and then we could consider the other ones later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## **2. Dataset Construction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will start by importing some of the pakages that we will used later on in the simulation part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from numpy import linalg as LA\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 2.1 System Model Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to be able to run some numerical simulations, construct the neural network, and tune its parameters, we need to sepecify the parameters that characterize the environment of interest, and start constructing the dataset that will be used to train the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beta = 3.76 # 3GPP propogation environment\n",
    "sigma = 1 # variance of the noise\n",
    "sigma_alpha_db = 7 # variance of $\\alpha$ in dB\n",
    "sigma_alpha = 10**(sigma_alpha_db/10)\n",
    "N = 4 # number of antennas in the BS\n",
    "K = 10 # number of users (K >= N)\n",
    "X0 = 0.500 # parameter of the uniform distribution of the user on the x-axis\n",
    "Y0 = 0.500 # parameter of the uniform distribution of the user on the x-axis\n",
    "nsize = 10000 # size of the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 2.2 Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function `clusters`. This function takes in a set A and gives all possible (two) clusters (without repetition) from a set A except the set A and empty set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def clusters(A):\n",
    "    subsets = []\n",
    "    for i in range(1, len(A)):\n",
    "        subsets.append(list(combinations(A,i)))\n",
    "    combos = []\n",
    "    for i in range(1, int(1+len(subsets)/2)):\n",
    "        combos.extend(list(product(subsets[i-1], subsets[-i])))\n",
    "    if not len(A) % 2:\n",
    "        combos.extend(list(combinations(subsets[int(len(A)/2)-1], 2)))\n",
    "    return [combo for combo in combos if not set(combo[0]) & set(combo[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test this function on an example to see how it works. Let's consider a set $A = \\{1, 2, 3\\}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[((1,), (2, 3, 4)), ((2,), (1, 3, 4)), ((3,), (1, 2, 4)), ((4,), (1, 2, 3)), ((1, 2), (3, 4)), ((1, 3), (2, 4)), ((1, 4), (2, 3))]\n"
     ]
    }
   ],
   "source": [
    "A = {1, 2, 3, 4}\n",
    "print(clusters(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As you can see, this give us all possible clusters from $A$ without repetition and excluding the cluster $(A, \\emptyset)$. Later on, since the order of the cluster matters, we will need to take into consideration the other possibilities where we need to just flip the elements of each tuple and also add the trivial cluster $(A, \\emptyset)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we will define a function that generates a sample channel matrix $\\textbf{H}$ given the parameters $N$ and $K$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def channel_matrix(N, K):\n",
    "    \n",
    "    d = 0.5\n",
    "    AoA=np.random.uniform(-0.5*np.pi,0.5*np.pi,size=(1,K))      \n",
    "    # Uncomment this if you want AoAs sorted in asccending order when generated\n",
    "    #AoA=np.sort(AoA)   \n",
    "    AoA_deg=180.0/np.pi*AoA    \n",
    "    phase_shifts_gen=2*np.pi*d*np.sin(AoA)    \n",
    "    aux=np.reshape(np.arange(0,N),(N,1))    \n",
    "    phase_shifts=np.dot(aux,phase_shifts_gen)   \n",
    "    amplitudes=np.ones((1,K))    \n",
    "    amplitudes_extd = np.dot(np.ones((N,1)), amplitudes)   \n",
    "    H=np.multiply(amplitudes_extd, np.exp(-1j*phase_shifts)) #Element by element product.\n",
    "    \n",
    "    return H"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "def channel_matrix(N, K):\n",
    "    H = np.zeros([N,K], dtype=np.complex_)\n",
    "    for i in range(K):\n",
    "        H[:,i] = np.sqrt(0.5)*(np.random.randn(N) + 1j*np.random.randn(N))\n",
    "    return H\n",
    "\n",
    "# H.real, H.imag, np.angle(H) real, imaginary and angle parts of H.\n",
    "# For the angle, one can use np.arctan(H.imag/H.real) also."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test this function! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.        +0.j        ,  1.        +0.j        ,\n",
       "         1.        +0.j        ,  1.        +0.j        ,\n",
       "         1.        +0.j        ,  1.        +0.j        ,\n",
       "         1.        +0.j        ,  1.        +0.j        ,\n",
       "         1.        +0.j        ,  1.        +0.j        ],\n",
       "       [-0.98175724-0.19013868j,  0.9029751 +0.42969289j,\n",
       "        -0.20593392+0.9785659j , -0.81386709-0.58105109j,\n",
       "         0.25031341+0.96816486j, -0.81126329+0.58468101j,\n",
       "         0.99976682+0.02159391j, -0.96149039-0.27483854j,\n",
       "         0.54716974+0.83702167j,  0.85826555+0.51320584j],\n",
       "       [ 0.92769457+0.37334005j,  0.63072805+0.77600395j,\n",
       "        -0.91518244-0.40303982j,  0.32475927+0.94579671j,\n",
       "        -0.87468639+0.4846893j ,  0.31629624-0.94866047j,\n",
       "         0.99906741+0.04317775j,  0.84892755+0.52850924j,\n",
       "        -0.40121055+0.91598586j,  0.47323952+0.8809338j ],\n",
       "       [-0.83978447-0.54291992j,  0.23608834+0.9717316j ,\n",
       "         0.58286813-0.81256676j,  0.28524533-0.95845454j,\n",
       "        -0.68820489-0.72551639j,  0.29806423+0.95454582j,\n",
       "         0.99790207+0.06474145j, -0.67098098-0.74147456j,\n",
       "        -0.98623029+0.16537782j, -0.04593519+0.99894442j]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "H = channel_matrix(N, K)\n",
    "H"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we will define the function `power_matrix` that generates a sample of the diagonal elements of the power matrix $\\bf{P}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def power_matrix(X0, Y0, K, sigma_alpha):\n",
    "    alpha = sigma_alpha*np.random.randn(K)\n",
    "    X = np.random.uniform(low=0.0, high=X0, size=K)\n",
    "    Y = np.random.uniform(low=0.0, high=Y0, size=K)\n",
    "    d = np.sqrt(X**2+Y**2)\n",
    "    P = 10**(alpha/10)*d**(-beta)\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 82.11804719,  97.90910592,  95.76338197,  32.92304463,\n",
       "       153.21237372, 397.41479225, 800.23770596,   2.43234593,\n",
       "        15.45701145,   1.72570267])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = power_matrix(X0, Y0, K, sigma_alpha)\n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the function gives only the diagonal elements of $\\bf{P}$, later on, we will build the corresponding matrix from these diagonal elements. Since we will use hermetian matrices, a small function `hermetian` is implemented for this purpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def hermetian(H):\n",
    "    return H.conj().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now, we will define a function that computes $SNR_i^{(1)}$, the SNR of user $i$ in the first cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def SNR1(H, P, sigma, i):\n",
    "    K = H.shape[1]\n",
    "    N = H.shape[0]\n",
    "    P_i= P[i]\n",
    "    h_i = H[:,i][:, np.newaxis]\n",
    "    P = np.delete(P, i)\n",
    "    set_minus_i = list(range(K))\n",
    "    set_minus_i.pop(i)\n",
    "    SNR = P_i*hermetian(h_i).dot(inv(H[:,set_minus_i].dot(np.diag(P)).dot(hermetian(H[:,set_minus_i])) + sigma**2*np.identity(N))).dot(h_i)\n",
    "    return SNR.real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Similarly, using the formula explained above, we implement a function that computes $SNR_i^{(2)}$, the SNR of user $i$ in the second cluster, give the set of user of the first cluster *set_I1*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def SNR2(H, P, sigma, i, set_I1):\n",
    "    K = H.shape[1]\n",
    "    N = H.shape[0]\n",
    "    P_i= P[i]\n",
    "    set_0 = set_I1 + [i]\n",
    "    h_i = H[:,i][:, np.newaxis]\n",
    "    P = np.delete(P, set_0)\n",
    "    set_minus_i = list(range(K))\n",
    "    set_minus_i = [j for j in set_minus_i if j not in set_0]\n",
    "    SNR = P_i*hermetian(h_i).dot(inv(H[:,set_minus_i].dot(np.diag(P)).dot(hermetian(H[:,set_minus_i])) + sigma**2*np.identity(N))).dot(h_i)\n",
    "    return SNR.real"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function `labelling` is used to determine the best clustering strategy given a certain criteria (e.g., maximum of sum rate). This function uses exhuastive search to determine the clustering strategy and will be used to label the examples in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def labelling(N, K, H, P):\n",
    "    A = set(range(K))\n",
    "    set_total = clusters(A)\n",
    "    set_total.append((tuple(A),()))\n",
    "    set_1 = [list(set_total[i][0]) for i in range(2**(K-1))]\n",
    "    set_2 = [list(set_total[i][1]) for i in range(2**(K-1))]\n",
    "    set_I1 = set_1 + set_2\n",
    "    set_I2 = set_2 + set_1\n",
    "    R_1 = np.zeros(2**K)\n",
    "    R_2 = np.zeros(2**K)\n",
    "    for i in range(2**K): #each possible scenario\n",
    "        SNR_1 = [SNR1(H, P, sigma, s) for s in set_I1[i]] # SNR_i^(1) for i in I_1\n",
    "        SNR_2 = [SNR2(H, P, sigma, s, set_I1[i]) for s in set_I2[i]] # SNR_i^(2) for i in I_2\n",
    "        R_1[i] = np.sum(np.log(1+np.array(SNR_1)))\n",
    "        R_2[i] = np.sum(np.log(1+np.array(SNR_2)))\n",
    "    # define the criterion to select the best configuration (max(R1 + R2) or fairness: max(min(R1,R2)) or max R1*R2)\n",
    "    sum_rate = R_1 + R_2\n",
    "    best_index = np.argmax(sum_rate)\n",
    "    return set_I1[best_index], set_I2[best_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function `DataSet` is used to construct the dataset used to train the neural network. We will save, for each scenario, i.e., each sampled $\\bf{H}$ and $\\bf{P}$, the matrix $\\bf{H \\times \\sqrt{P}}$ in a folder (already created) called `Data` as h5py file. In other words, the folder `Data` will contain *nsize* h5py files once the function finish running. Later on, we will use the real, imaginary and angle parts of each instance as stack them in one matrice to be fed as the input of the neural netwrok. We will get also a dataframe as a csv file containing for each matrix $\\bf{H \\times \\sqrt{P}}$ , the corresponding clustering strategy as a $1 \\times K$ vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def DataSet(N, K, X0, Y0, sigma_alpha, nsize):\n",
    "    # creating empty dataframe with columns\n",
    "    col_names=['Power' + str(i+1) for i in range(K)] + ['User' + str(i+1) for i in range(K)]\n",
    "    column_names=['H'] + ['P'] + ['User' + str(i+1) for i in range(K)]\n",
    "    df = pd.DataFrame(columns = column_names, index = range(nsize))\n",
    "    df_power = pd.DataFrame(columns = col_names, index = range(nsize))\n",
    "    for j in range(nsize):\n",
    "        \n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        H = channel_matrix(N, K)\n",
    "        P = power_matrix(X0, Y0, K, sigma_alpha)\n",
    "        \n",
    "        power = []\n",
    "        for i in range(K):\n",
    "            h_i = H[:,i][:, np.newaxis]\n",
    "            P_i = P[i]\n",
    "            power.append(LA.norm(h_i)**2*P_i)\n",
    "        \n",
    "        # store the matrices for each scenario\n",
    "        with h5py.File('Data/H_'+ str(j+1) +'.h5', 'w') as hf:\n",
    "            hf.create_dataset(\"H_\" + str(j+1),  data=H)\n",
    "        \n",
    "        with h5py.File('Data/P_'+ str(j+1) +'.h5', 'w') as hf:\n",
    "            hf.create_dataset(\"P_\" + str(j+1),  data=np.diag(P))\n",
    "                    \n",
    "        # get the best clustering strategy\n",
    "        set0, set1 = labelling(N, K, H, P)\n",
    "        # transform it to one vector\n",
    "        labels = np.zeros(K)\n",
    "        for i in range(K):\n",
    "            if  i in set0:\n",
    "                labels[i] = 0\n",
    "            else:\n",
    "                labels[i] = 1\n",
    "        \n",
    "        df.loc[j] = ['H_'+ str(j+1) +'.h5'] + ['P_'+ str(j+1) +'.h5'] + list(labels)\n",
    "        df_power.loc[j] = list(power) + list(labels)\n",
    "        print('Progress:', np.round((j+1)/nsize*100,2), '%')\n",
    "    \n",
    "    df.to_csv('data.csv', encoding='utf-8')\n",
    "    df_power.to_csv('data_power.csv', encoding='utf-8')\n",
    "    return df, df_power"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's test this function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = DataSet(N, K, X0, Y0, sigma_alpha, nsize)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
