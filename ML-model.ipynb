{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explain the ML strategy used to make the classification of users. We used a transformation method `Classifier Chains` with a base classifier, light GBM, a boosted-tree algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 4 # number of antennas in the BS\n",
    "K = 10 # number of users (K >= N)\n",
    "X0 = 0.500 # parameter of the uniform distribution of the user on the x-axis\n",
    "Y0 = 0.500 # parameter of the uniform distribution of the user on the y-axis\n",
    "nsize = 10000 # size of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product, combinations\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import h5py\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from numpy import linalg as LA\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import seaborn as sns\n",
    "from sklearn import metrics\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "np.random.seed(42) # fix the seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_matrices_sizes(matrix_name):\n",
    "    with h5py.File('Data/' + matrix_name, 'r') as hf:\n",
    "        mat = hf[matrix_name[:-3]][:]\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the matrix is (4, 10)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "        0.00000000e+00,  0.00000000e+00,  4.50685925e-01, -7.99110965e-03,\n",
       "       -9.15351277e-03, -2.19030685e+00, -4.84237626e+00, -7.31735971e+00,\n",
       "        3.91558025e+00, -7.86676350e+00, -2.75607383e+00, -3.00867086e+00,\n",
       "        8.67232976e-01,  1.59822045e-02,  1.83069974e-02,  4.05950376e+00,\n",
       "        7.56442256e+00, -1.03855771e+01,  3.92323412e+00, -2.27582610e-01,\n",
       "       -2.13146035e+00, -4.76236947e+00,  1.21808825e+00, -2.39732699e-02,\n",
       "       -2.74604259e-02, -5.33355712e+00, -6.97423720e+00, -7.42295873e+00,\n",
       "        1.53226998e-02,  7.86017962e+00,  1.10766980e+00, -4.52959570e+00])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix_name = df['H'][0]\n",
    "H = read_matrices_sizes(matrix_name)\n",
    "matrix_name = df['P'][0]\n",
    "P = read_matrices_sizes(matrix_name)\n",
    "mat = H.dot(np.sqrt(P))\n",
    "print('The shape of the matrix is', mat.shape)\n",
    "mat.imag.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data(df):\n",
    "    with h5py.File('Data/' + df['H'], 'r') as hf:\n",
    "        H = hf[df['H'][:-3]][:]\n",
    "    with h5py.File('Data/' + df['P'], 'r') as hf:\n",
    "        P = hf[df['P'][:-3]][:]\n",
    "    N = H.shape[0]\n",
    "    K = H.shape[1]\n",
    "    x = np.zeros([N, K, 2])\n",
    "    mat = H.dot(np.sqrt(P))\n",
    "    x[:,:,0] = mat.real\n",
    "    x[:,:,1] = mat.imag\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Matrix'] = df.apply(transform_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 10, 2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['Matrix'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(df):\n",
    "    X = np.stack(df['Matrix'].values)\n",
    "    cols = ['User' + str(i+1) for i in range(K)]\n",
    "    y = df[cols].values\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_size = 0.2\n",
    "random_state = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=valid_size, random_state=random_state)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(\"Train set rows: {}\".format(train_df.shape[0]))\n",
    "print(\"Validtion set rows: {}\".format(valid_df.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = encoder(train_df)\n",
    "X_valid, y_valid = encoder(valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((8000, 4, 10, 2), (8000, 10))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.zeros((X_train.shape[0],N*K*2))\n",
    "for i in range(X_train.shape[0]):\n",
    "    X_tr[i, :] = np.concatenate((X_train[i,:,:,0].flatten(), X_train[i,:,:,1].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val = np.zeros((X_valid.shape[0],N*K*2))\n",
    "for i in range(X_valid.shape[0]):\n",
    "    X_val[i, :] = np.concatenate((X_valid[i,:,:,0].flatten(), X_valid[i,:,:,1].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_tr = sc.fit_transform(X_tr)\n",
    "X_val= sc.transform(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 12)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transf_data(df):\n",
    "    with h5py.File('test_Data/' + df['H'], 'r') as hf:\n",
    "        H = hf[df['H'][:-3]][:]\n",
    "    with h5py.File('test_Data/' + df['P'], 'r') as hf:\n",
    "        P = hf[df['P'][:-3]][:]\n",
    "    N = H.shape[0]\n",
    "    K = H.shape[1]\n",
    "    x = np.zeros([N, K, 2])\n",
    "    mat = H.dot(np.sqrt(P))\n",
    "    x[:,:,0] = mat.real\n",
    "    x[:,:,1] = mat.imag\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['Matrix'] = test_df.apply(transf_data,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = encoder(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 4, 10, 2), (2000, 10))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = np.zeros((X_test.shape[0],N*K*2))\n",
    "for i in range(X_test.shape[0]):\n",
    "    X_te[i, :] = np.concatenate((X_test[i,:,:,0].flatten(), X_test[i,:,:,1].flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_te = sc.transform(X_te)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In traditional classification, the standard evaluation criterion is the accuracy. In multilabel classification, a direct extension of the accuracy is the exact match ratio, which considers one instance as correct if and only if all associated labels are correctly predicted. However, this ratio may not be the most suitable performance measure as it does not count partial matches. The metrics can be divided in two categories:\n",
    "\n",
    "-  Example-Based Evaluation Methods: exact math ratio, hamming loss, jaccard similarity index, etc,\n",
    "-  Label-Based Evaluation Methods: $F_1$ score, precision, recall, etc.\n",
    "\n",
    "**1. Exact Match Ratio**\n",
    "\n",
    "Assume there are $L$ testing instances. Let $y^i$ be the true label vector of the $i^{th}$ instance and $\\hat{y}^i$ be the predicted label vector, then the exact match ratio (EMR) is defined as\n",
    "$$ EMR = \\frac{1}{L} \\sum_{i=1}^{L}{\\mathbb{1}\\left[\\hat{y}^i = y^i\\right]} $$\n",
    "\n",
    "**2. Micro and Macro $F_1$ score, precision, and recall**\n",
    "\n",
    "One of the most used performance measures for information retrieval systems is the F-measure, which is the harmonic mean of precision (P) and recall (R):\n",
    "$$ F = \\frac{2 P R}{P + R} $$\n",
    "\n",
    "Precision is the measure of how much the method is immune to Type I error, i.e., falsely classifying negative cases as positives: false positives or FP. It is the fraction of correctly positively-classified cases (i.e., true positives) to all positively-classified cases.\n",
    "\n",
    "Recall is the measure of how much the method is immune to the Type II error, i.e., falsely classifying positive cases as negatives: false negatives or FN. It is the fraction of correctly positively-classified cases (i.e., true positives) to all positively-classified label.\n",
    "\n",
    "Due to a natural non-uniformity of the distribution of labels among input objects in any testing set. Two averaging techniques exist:\n",
    "\n",
    "- Micro-averaging gives equal weight to every input object and performs a global aggregation of true/false positives/negatives, averaging over all objects first, \n",
    "- Macro-averaging, the measure is first calculated per label, then averaged over the number of labels. Macro averaging thus gives equal weight to each label, regardless of how often the label appears.\n",
    "\n",
    "**3. Hamming loss**\n",
    "\n",
    "Hamming loss is a label-wise decomposable function counting the fraction of labels that were misclassified.\n",
    "\n",
    "**4. Jaccard Similarity Index**\n",
    "\n",
    "Jaccard similarity is a measure of the size of similarity between the prediction and the ground truth comparing what is the cardinality of an intersection of the two, compared to the union of the two. In other words, what fraction of all labels taken into account by any of the prediction or ground truth were assigned to the observation in both of the cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset, ClassifierChain\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, hamming_loss, jaccard_score, f1_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `find_best_threshold` finds the best threshold, tuned in the validation stage, for the purpose of maximizing a certain criterion (one of the evaluation metrics)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_threshold(classifier, X, y):\n",
    "    \n",
    "    out = classifier.predict_proba(X)\n",
    "\n",
    "    threshold = np.arange(0.1,0.9,0.1)\n",
    "\n",
    "    acc = []\n",
    "    accuracies = []\n",
    "    best_threshold = np.zeros(out.shape[1])\n",
    "    for i in range(out.shape[1]):\n",
    "        y_prob = out[:,i]\n",
    "        for j in threshold:\n",
    "            y_pred = [1 if prob>=j else 0 for prob in y_prob]\n",
    "            acc.append(accuracy_score(y[:,i],y_pred)) # criterion definition\n",
    "        acc   = np.array(acc)\n",
    "        index = np.where(acc==acc.max()) \n",
    "        accuracies.append(acc.max()) \n",
    "        best_threshold[i] = threshold[index[0][0]]\n",
    "        acc = []\n",
    "    return out, best_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores(y1, y2):\n",
    "    scores = [accuracy_score(y1, y2), hamming_loss(y1, y2), \n",
    "           jaccard_score(y1, y2, average='samples'), \n",
    "           f1_score(y1, y2, average='samples')]\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_valid = pd.DataFrame(columns=['Method', 'Accuracy', 'Hamming Loss', 'Jaccard Score', 'F_1 Score'])\n",
    "df_scores_test = pd.DataFrame(columns=['Method', 'Accuracy', 'Hamming Loss', 'Jaccard Score', 'F_1 Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hermetian(H):\n",
    "    return H.conj().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR1(H, P, sigma, i):\n",
    "    K = H.shape[1]\n",
    "    N = H.shape[0]\n",
    "    P_i= P[i]\n",
    "    h_i = H[:,i][:, np.newaxis]\n",
    "    P = np.delete(P, i)\n",
    "    set_minus_i = list(range(K))\n",
    "    set_minus_i.pop(i)\n",
    "    SNR = P_i*hermetian(h_i).dot(inv(H[:,set_minus_i].dot(np.diag(P)).dot(hermetian(H[:,set_minus_i])) + sigma**2*np.identity(N))).dot(h_i)\n",
    "    return SNR.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SNR2(H, P, sigma, i, set_I1):\n",
    "    K = H.shape[1]\n",
    "    N = H.shape[0]\n",
    "    P_i= P[i]\n",
    "    set_0 = set_I1 + [i]\n",
    "    h_i = H[:,i][:, np.newaxis]\n",
    "    P = np.delete(P, set_0)\n",
    "    set_minus_i = list(range(K))\n",
    "    set_minus_i = [j for j in set_minus_i if j not in set_0]\n",
    "    SNR = P_i*hermetian(h_i).dot(inv(H[:,set_minus_i].dot(np.diag(P)).dot(hermetian(H[:,set_minus_i])) + sigma**2*np.identity(N))).dot(h_i)\n",
    "    return SNR.real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_rate(df):\n",
    "    sigma = 1\n",
    "    with h5py.File('test_Data/' + df['H'], 'r') as hf:\n",
    "        H = hf[df['H'][:-3]][:]\n",
    "    with h5py.File('test_Data/' + df['P'], 'r') as hf:\n",
    "        P = hf[df['P'][:-3]][:]\n",
    "    P_diag = P.diagonal()\n",
    "    N = H.shape[0]\n",
    "    K = H.shape[1]\n",
    "    cols = ['User'+str(i+1) for i in range(K)]\n",
    "    conf = df[cols].values\n",
    "    set_I1 = np.where(conf == 0)[0].tolist()\n",
    "    set_I2 = np.where(conf == 1)[0].tolist()\n",
    "    SNR_1 = [SNR1(H, P_diag, sigma, s) for s in set_I1] # SNR_i^(1) for i in I_1\n",
    "    SNR_2 = [SNR2(H, P_diag, sigma, s, set_I1) for s in set_I2] # SNR_i^(2) for i in I_2\n",
    "    R_1 = np.sum(np.log(1+np.array(SNR_1)))\n",
    "    R_2 = np.sum(np.log(1+np.array(SNR_2)))\n",
    "    return R_1 + R_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test_df = pd.DataFrame(columns=test_df.columns)\n",
    "pred_test_df['H'] = test_df['H']\n",
    "pred_test_df['P'] = test_df['P']\n",
    "pred_test_df['Matrix'] = test_df['Matrix']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['User' + str(i+1) for i in range(K)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "    'classifier': [lgb.LGBMClassifier()],\n",
    "    'classifier__learning_rate': [0.005],\n",
    "    'classifier__n_estimators': [40],\n",
    "    'classifier__num_leaves': [6,8,12,16],\n",
    "    'classifier__boosting_type' : ['gbdt'],\n",
    "    'classifier__objective' : ['binary'],\n",
    "    'classifier__random_state' : [501], # Updated from 'seed'\n",
    "    'classifier__colsample_bytree' : [0.65, 0.66],\n",
    "    'classifier__subsample' : [0.7,0.75],\n",
    "    'classifier__reg_alpha' : [1,1.2],\n",
    "    'classifier__reg_lambda' : [1,1.2,1.4],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n",
      "Finished loading model, total used 100 iterations\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "classifier = ClassifierChain(\n",
    "    classifier = lgb.LGBMClassifier(n_estimators = 100, learning_rate = 0.1),\n",
    "    #classifier = RandomForestClassifier(n_estimators=100),\n",
    "    require_dense = [False, True]\n",
    ")\n",
    "\n",
    "classifier.fit(X_tr, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = classifier.predict_proba(X_te)\n",
    "y_pred = np.array([[1 if out[i,j]>=0.5 else 0 for j in range(y_test.shape[1])] for i in range(len(y_test))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Hamming Loss</th>\n",
       "      <th>Jaccard Score</th>\n",
       "      <th>F_1 Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>CC-LGBM</td>\n",
       "      <td>0.062</td>\n",
       "      <td>0.29095</td>\n",
       "      <td>0.465306</td>\n",
       "      <td>0.605706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Method  Accuracy  Hamming Loss  Jaccard Score  F_1 Score\n",
       "0  CC-LGBM     0.062       0.29095       0.465306   0.605706"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_scores_test.loc[df_scores_test.shape[0]] = ['CC-LGBM'] + list(scores(y_test, y_pred))\n",
    "df_scores_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>H</th>\n",
       "      <th>P</th>\n",
       "      <th>User1</th>\n",
       "      <th>User2</th>\n",
       "      <th>User3</th>\n",
       "      <th>User4</th>\n",
       "      <th>User5</th>\n",
       "      <th>User6</th>\n",
       "      <th>User7</th>\n",
       "      <th>User8</th>\n",
       "      <th>User9</th>\n",
       "      <th>User10</th>\n",
       "      <th>Matrix</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>H_1.h5</td>\n",
       "      <td>P_1.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[1.516401437104227, 0.0], [129.2895136471787...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>H_2.h5</td>\n",
       "      <td>P_2.h5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[15.807212199907328, 0.0], [5.22225822717810...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>H_3.h5</td>\n",
       "      <td>P_3.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[5.437638959647273, 0.0], [5.193951816534471...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>H_4.h5</td>\n",
       "      <td>P_4.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>[[[3.198971794472364, 0.0], [8.187352466856316...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>H_5.h5</td>\n",
       "      <td>P_5.h5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>[[[2.720206087097921, 0.0], [7.87050437793912,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        H       P User1 User2 User3 User4 User5 User6 User7 User8 User9  \\\n",
       "0  H_1.h5  P_1.h5     0     1     0     0     1     0     0     0     1   \n",
       "1  H_2.h5  P_2.h5     1     0     0     0     1     0     1     1     0   \n",
       "2  H_3.h5  P_3.h5     0     0     1     0     0     1     1     0     0   \n",
       "3  H_4.h5  P_4.h5     0     1     0     0     1     1     0     0     0   \n",
       "4  H_5.h5  P_5.h5     0     1     0     0     0     0     1     1     0   \n",
       "\n",
       "  User10                                             Matrix  \n",
       "0      1  [[[1.516401437104227, 0.0], [129.2895136471787...  \n",
       "1      0  [[[15.807212199907328, 0.0], [5.22225822717810...  \n",
       "2      1  [[[5.437638959647273, 0.0], [5.193951816534471...  \n",
       "3      0  [[[3.198971794472364, 0.0], [8.187352466856316...  \n",
       "4      1  [[[2.720206087097921, 0.0], [7.87050437793912,...  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_test_df[cols] = y_pred\n",
    "pred_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['sum_rate'] = test_df.apply(sum_rate, axis = 1)\n",
    "pred_test_df['sum_rate'] = pred_test_df.apply(sum_rate, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of the sum rate on the test set is: 5.68\n",
      "MAPE of the sum rate on the test set is: 17.74\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "rmse = np.sqrt(mean_squared_error(test_df['sum_rate'],pred_test_df['sum_rate']))\n",
    "mape = np.mean(np.abs((test_df['sum_rate'] - pred_test_df['sum_rate']) / test_df['sum_rate'])) * 100\n",
    "print('RMSE of the sum rate on the test set is:', round(rmse,2))\n",
    "print('MAPE of the sum rate on the test set is:', round(mape,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's time the prediction of one instance using the proposed ML model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "total = []\n",
    "for i in range(10):\n",
    "    t0 = time.time()\n",
    "    y_pred = classifier.predict(X_te[1,:])\n",
    "    t1 = time.time()\n",
    "    total.append(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024399924278259277"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(total)/10"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
